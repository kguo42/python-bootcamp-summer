{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar 3: File IO, Pandas and Plotting\n",
    "\n",
    "In seminar 2 we covered the basics of conditional statements and how to implement them into your code. We also covered how to make your own functions and covered the difference between local and global variables and how to use plain english to jot down the steps needed to achieve an objective so that you can then translate the plain english plan into code later. \n",
    "\n",
    "Quick review of Seminar 2 concepts:\n",
    "\n",
    "If-Else Statements\n",
    "    \n",
    "    if condition1:\n",
    "        Execute Code for Condition 1 Here\n",
    "    elif condition2:\n",
    "        Execute Code for Condition 2 Here\n",
    "    else:\n",
    "        Execute Code if neither condition is met\n",
    "        \n",
    "For-Loops:\n",
    "    \n",
    "    #this code iterates over the values of iterable \n",
    "    for variable_name in iterable:\n",
    "        CODE FOR FOR-LOOP\n",
    "    \n",
    "    #this for-loop loops over the indices of the array iterable\n",
    "    for idx in range(len(iterable)):\n",
    "        CODE FOR FOR-LOOP\n",
    "    \n",
    "Custom Functions\n",
    "    \n",
    "    def FUNCTION_NAME(ARGUMENTS (if needed):\n",
    "        '''\n",
    "        DOCSTRINGS GOES HERE\n",
    "        '''\n",
    "        \n",
    "        CODE FOR FUNCTION\n",
    "        \n",
    "        return RETURN_VARIABLE (if needed)\n",
    "        \n",
    "Flowcharts and Pseudocode\n",
    "\n",
    "    What is the Code Objective? (ie: What do I want my code to do?)\n",
    "    What information do I have available to me? (ie: some files, maybe an equation(s))\n",
    "    What are some coding tools I have to achieve the Code Objective?(if-else, File IO, plotting, custom function)\n",
    "    \n",
    "    Answering the above questions will be crucial for your flowchart and pseudocode as \n",
    "    this will help guide you into what needs to be done and then allow you to translate the written \n",
    "    statements into coding syntax\n",
    "    \n",
    "   \n",
    "Now we will pivot into a very important section of coding for research and this is learning how to read in files. In research we are always dealing with files and sending out files to our advisors or collaborators. So knowing how to open a file and grab the relavent data you will need is extremely important for your success as a researcher. We will cover how to open basic text files and csv files in this notebook, in Seminar 4 we will cover how to open and access FITS files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in Text Files\n",
    "\n",
    "One of the most common files to send and store information are text files. They are easy to open, make and luckily for us there are plenty of function that we can use to access the contents of text files. \n",
    "\n",
    "Python has a built in function that can open all kinds of files. It is super versatile and when used well can be an amazing tool to open even the most complicated file format. The python function to read in files is the $\\textbf{open}$ function. We will cover the basic use of it and how it can be used for reading and generating new files. The syntax for reading in a file using open is as follows:\n",
    "\n",
    "    file = open('PATH/TO/FILE/FILENAME.txt', 'r')\n",
    "    \n",
    "\n",
    "The 'r' is important here as it tell python that we want to read the file only and do not want to write anything to the file. So the variable file will only have things to do with reading in the file. \n",
    "\n",
    "The variable file is now a textwrapper and contains within it the entire entries of the file. We isolate each row of the file by looping over the contents of file using a $\\textbf{For-Loop}$ or using one of the access function that file has."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Reading in the file Galaxy_Coordinates.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to read in the file\n",
    "file = open('Galaxy_Coordinates.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's see what functions we have available to us\n",
    "file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code that grabs all the lines in the file and stores it to the variable lines\n",
    "lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#always a good practice to close the file so that it does not take up memory in the jupyter notebook\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code that loops through all the lines in the file line-by-line\n",
    "for row in lines:\n",
    "    print(f'Row: {row} Type: {type(row)} --- Length of the Row: {len(row)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A quick guide on string splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row1 = lines[0]\n",
    "row1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filename = 'Unique_Filename_Number'\n",
    "test_filename.split('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = 'Usr/Desktop/Python_BootCamp/Seminar3/File.txt'\n",
    "test_path.split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making empty arrays to later store data from the file here\n",
    "col1 = np.array([])\n",
    "col2 = np.array([])\n",
    "col3 = np.array([])\n",
    "col4 = np.array([])\n",
    "\n",
    "#looping over all the rows except the first as the first row contains the column names\n",
    "for row in lines[1:]:\n",
    "    \n",
    "\n",
    "    col1 = np.append(col1, row.split()[0])\n",
    "    col2 = np.append(col2, row.split()[1])\n",
    "    col3 = np.append(col3, row.split()[2])\n",
    "    col4 = np.append(col4, row.split()[3])\n",
    "        \n",
    "col1 = col1.astype(int)\n",
    "col2 = col2.astype(int)\n",
    "col3 = col3.astype(float)\n",
    "col4 = col4.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Column 1: {col1}, Type: {type(col1[0])}')\n",
    "print(f'Column 2: {col2}, Type: {type(col2[0])}')\n",
    "print(f'Column 3: {col3}, Type: {type(col3[0])}')\n",
    "print(f'Column 4: {col4}, Type: {type(col4[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing to a File\n",
    "\n",
    "Often times when we are doing research we want to save the output of our analysis to a file for later use. That way we do not need to redo the analysis but rather open up the file that has the subset of data we want. We can also use the $\\textbf{open}$ function to write out to a file and save the results. All we would change from the syntax is change the mode, instead of using the 'r' command when we are reading a file we will change it to 'w+' as this allows us to write to a file and make it if it does not exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Formatting to Write to Files\n",
    "\n",
    "One of the ways to write data to a file is by leveraging a tool in python called $\\textit{string formatting}$, string formatting is a way to make the string output look clean as you can have everything be lined up how you want. It can also help you keep a certain number of significant figures and can even have data in exponential form. To use this format on the variable *value* see below\n",
    "\n",
    "Basic Syntax:\n",
    "\n",
    "    f'STRING TO WRITE {value}'\n",
    "\n",
    "Advanced Syntax\"\n",
    "\n",
    "    f'STRING TO WRITE {value:.2f}' Converts data to a float and keeps it to 2 values after the decimal\n",
    "    f'STRING TO WRITE {value:.3f}' Converts data to a float and keeps it to 3 values after the decimal\n",
    "    f'STRING TO WRITE {value:.2e}' Converts data to a float and writes it in exponential form (ie: 1.23e-10)\n",
    "    f'STRING TO WRITE {value:.3e}' Converts data to a float and writes it in exponential form (ie: 1.233e-10)\n",
    "    \n",
    "    Number before the decimal tells python how many spaces it will occupy in total, if its able to\n",
    "    f'STRING TO WRITE {value:5.2f}' Converts data to a float and writes it in exponential form with 5 total spaces\n",
    "    f'STRING TO WRITE {value:5.3e}' Converts data to a float and writes it in exponential form with 5 total spaces\n",
    "    \n",
    "You can even add multiple string formatting into the string by adding in {} for every variable that you want to apply string formatting.   \n",
    "    \n",
    "    f'{data1:.2f} {data2:.3f} {data3:.2e}'\n",
    "    \n",
    "More info on f-string formatting can be found here: \n",
    "\n",
    "https://www.geeksforgeeks.org/formatted-string-literals-f-strings-python/#\n",
    "\n",
    "https://realpython.com/python-f-strings/ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = 0.0355"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'Value is: {value:.2f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'Value is: {value:.2e}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating random data to write to a file\n",
    "np.random.seed(100)\n",
    "\n",
    "wavelength = np.linspace(1000, 4500, 1000)\n",
    "flux = np.random.normal(loc = 3, scale = 5, size = 1000)\n",
    "flux_err = np.random.normal(loc = 1, scale = 1, size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code that makes a new file and generates the file handler variable spectra_file\n",
    "\n",
    "#opening up the file and using the file handler variable spectra_file\n",
    "spectra_file = open('Output_Spectra.txt', 'w+')\n",
    "\n",
    "#writing to a file using the file handler and the .write() command. NOTE the \\n (new line character) at the end\n",
    "\n",
    "#writing out the column names of the file first\n",
    "spectra_file.write(f'Wavelength Flux Flux_err \\n')\n",
    "\n",
    "#actual data\n",
    "for wave, Fnu, Fnu_err in zip(wavelength, flux, flux_err):\n",
    "    \n",
    "    spectra_file.write(f'{wave:.3f} {Fnu:.2f} {Fnu_err:.2f}\\n')\n",
    "    \n",
    "spectra_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caution: Overwriting Files\n",
    "\n",
    "Be extremely careful when you are writing out data to a file especially with similar file names as if you write out data to a file that already exist it $\\textbf{will overwrite}$ that file. There is no automatic updater or warning telling you about a duplicate file so be a bit more cautious when you are writing data to a file as it will overwrite the file if it already exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Numpy function to read and write files\n",
    "\n",
    "The open function is very useful but it can be really confusing on how to acquire the data, especially when the file has many data types. In the following code cells we cover how to use two numpy functions to open text files and cover a bit of their pros and cons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## np.loadtxt\n",
    "\n",
    "The first function we will cover to open text files is np.loadtxt and this is super convenient when you are dealing with a file that is all numerical data. This is because by default what np.loadtxt does is that it tries to load in the file as $\\textbf{floats}$. You can run into problems when you have a file where there are column names, or the data in a column is all strings when using np.loadtxt to read in the file but we will cover some work arounds for this.\n",
    "\n",
    "Basic Syntax to use np.loadtxt\n",
    "\n",
    "    data = np.loadtxt(FILE/PATH/TO/FILE/FILENAME)\n",
    "    \n",
    "    \n",
    "By default np.loadtxt will try to read in the file assuming it is all numeric data and convert it all to floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try running the cell below and see what you get\n",
    "data_loadtxt = np.loadtxt('Galaxy_Coordinates.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You may have gotten an error, open up the file and see what could be causing the issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loadtxt = np.loadtxt('Galaxy_Coordinates.txt', skiprows=1)\n",
    "#skiprows skips the the first entry which is the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID1, ID2, RA, DEC =  np.loadtxt('Galaxy_Coordinates.txt', skiprows=1, unpack = True)\n",
    "#unpacks grabs every column and returns them back individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## np.genfromtxt\n",
    "\n",
    "The next function we will cover to open text files is np.genfromtxt and this is a more general version of np.loadtxt. This is because this has the ability to read in data values (ints and floats) but can also read in strings and booleans. Let us use it in action.\n",
    "\n",
    "Basic Syntax to use np.genfromtxt\n",
    "\n",
    "    data = np.genfromtxt(FILE/PATH/TO/FILE/FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_genfromtxt = np.genfromtxt('Galaxy_Coordinates.txt', skip_header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID1, ID2, RA, DEC = np.genfromtxt('Galaxy_Coordinates.txt', skip_header=1, unpack = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening a more complex file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_data = np.loadtxt('Field_Coordinates.txt', \n",
    "                       skiprows=1, \n",
    "                       usecols=(0, 1, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_field = np.loadtxt('Field_Coordinates.txt', \n",
    "                        skiprows=1, \n",
    "                        dtype = str, \n",
    "                        usecols=(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = np.loadtxt('Field_Coordinates.txt', skiprows=1, dtype = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.genfromtxt('Field_Coordinates.txt', \n",
    "                  skip_header= 1, \n",
    "                  unpack = True, \n",
    "                  dtype = None, \n",
    "                  encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing out Row 1 of X\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{type(X[0][0])} {type(X[1][0])} {type(X[2][0])} {type(X[3][0])} {type(X[4][0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Pandas and Dataframes\n",
    "\n",
    "Pandas is a very cool package that allows you to manipulate data through a table. You can easily perform computations on these tables, quickly acquire a subset of data given a condition and allows for easy data access as well. They work very similar to Astropy tables as well so if you are comfortable with pandas and Dataframes, then astropy tables would be something you can easily get down as well. We will cover the different syntax for defining a DataFrame how to do dataframe manipulations, such as filtering and merging DataFrames. Lastly we will cover some of the pandas file IO so that you can use pandas to open and write all sorts of files with ease. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas DataFrames\n",
    "\n",
    "Pandas DataFrames are a very useful tool whenever you have a table that is organize dby row and columns. Pandas DataFrame has lots of built in functions that allow for ease of exploration and manipulation of a pandas DataFrame, let us look at what a DataFrame is and how we can use it to do astronomy research. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Defining DataFrame Using Arrays\n",
    "\n",
    "You can define a pandas data frame by first making an empty DataFrame and then adding in the column name and values one at a time to populate the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making an empty DataFrame\n",
    "DF_1d_arrays = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DF_1d_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(0, 10, 1)\n",
    "b = np.arange(150, 160, 1)\n",
    "c = np.arange(200, 210, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_1d_arrays['A'] = a\n",
    "print(DF_1d_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_1d_arrays['B'] = b\n",
    "print(DF_1d_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_1d_arrays['C'] = c\n",
    "print(DF_1d_arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Using Dictionaries\n",
    "\n",
    "Dictionaries are a pretty cool type of container and are a class in and of their own. The way dictionaries work is that dictionaries work on a key-value pair system. Where you need to provide the dictionary with the key to get the corresponding value. The key can be numerical or a string but it has to be unique. You cannot have multiple entries of the same key holding different values. What would happen in this case is that the prior assigned value would be overwritten. Lets see an example of dictionaries in action and how we can convert them into pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_dictionary = {'A': a, \n",
    "                 'B': b, \n",
    "                 'C': c}\n",
    "\n",
    "DF = pd.DataFrame(DF_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Using 2D-Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_d_array = np.random.uniform(low = -100, high = 100, size = (100, 3))\n",
    "df_twod_arr = pd.DataFrame(two_d_array, columns= ['A', 'B', 'C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twod_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames are Fancy Numpy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 * DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF + 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF/DF.loc[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing and Changing Data in DataFrame\n",
    "\n",
    "Pandas DataFrame have two ways to access data and that is through the $\\textbf{.loc}$ or $\\textbf{.iloc}$ command and there are key differences between the two. In short $\\textbf{.loc}$ is able to use the index name and column names to select the exact entry that you want from the DataFrame where $\\textbf{.iloc}$ uses the index location entry similar to array indexing in the rows and columns to grab the data you are after. The general syntax for accessing data is the following:\n",
    "\n",
    "DF.loc[[index1, index2, index3, ..., index_n], [Column1, Column2, ..., Column_m]]\n",
    "\n",
    "DF.iloc[[index_idx1, index_idx2, ..., index_idx_n], [col_index1, col_index2, ..., col_index_m]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Value at row 4 and column B: {DF.loc[3, 'B']}\")\n",
    "print()\n",
    "print(f\"Value at row 4 and column A: {DF.loc[3, 'A']}\")\n",
    "print()\n",
    "print('Values in Rows 1-5 in Column A:')\n",
    "print(DF.loc[[0, 1, 2, 3, 4], 'A'])\n",
    "print()\n",
    "print('Values in rows 1-5 in Columns A, B, C:')\n",
    "print(DF.loc[[0, 1, 2, 3, 4], ['A', 'B', 'C']])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the 5th row and Column 1 and 3 to 5000\n",
    "DF.loc[4, ['A', 'C']] = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.loc[[0, 1, 2, 3, 4]] = 100005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick Excercise: \n",
    "#you find that the data from row 8 and column 1 and 2 are invalid due to \n",
    "#bad equipment to account for this change the values to the new value of -999\n",
    "\n",
    "#INSERT CODE BELOW\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same output as above but using iloc\n",
    "print(f\"Value at row 4 and column B: {DF.iloc[3, 1]}\")\n",
    "print(f\"Value at row 4 and column A: {DF.iloc[3, 0]}\")\n",
    "print()\n",
    "print('Values in Rows 1-5 in Column A:')\n",
    "print(DF.iloc[[0, 1, 2, 3, 4], 0])\n",
    "print()\n",
    "print('Values in rows 1-5 in Columns A, B, C:')\n",
    "print(DF.iloc[[0, 1, 2, 3, 4], [0, 1, 2]])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Pandas Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hipparcos_df = pd.read_csv('NSF_REU_File.txt', sep = ' ',\n",
    "                           index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful pandas functions in the cells below\n",
    "\n",
    "# Shows the first 5 entries of the DF, \n",
    "# useful to see what's the layout of the DF\n",
    "hipparcos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple description of the DF per column\n",
    "hipparcos_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many Nan values are in each column\n",
    "hipparcos_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shows all the columns in the DF\n",
    "hipparcos_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shows the index values of the DF\n",
    "hipparcos_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accessing data using .loc\n",
    "hipparcos_df.loc[[1, 1000, 11000], \n",
    "                 ['ra', 'dec', 'detectid', 'plya_classification']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handeling Multiple Data Frames\n",
    "\n",
    "In this section we will cover some ways of handeling multiple DataFrames. \n",
    "\n",
    "There are two main ways of merging two DataFrames and that is through the *concat* or the *join* function in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating Data\n",
    "DF1 = pd.DataFrame({'Field': ['Aegis', \"Aegis\", 'COSMOS', 'UDS', 'GOODS-N', 'GOODS-N'], \n",
    "                    \"RA\": [15.67, 16.54, 270.21, 50.00, 100.23, 101.22], \n",
    "                    'DEC': [5.34, 6.54, 76.21, 20.23, 80.23, 79.22]}, \n",
    "                    index = [145, 112, 198, 43, 76, 31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating Data 2\n",
    "np.random.seed(101)\n",
    "DF2 = pd.DataFrame({'f_g': np.random.normal(loc=5, scale = 2, size = 200), \n",
    "                    'e_g': np.abs(np.random.normal(0, scale = 1, size = 200)), \n",
    "                    'f_r': np.random.normal(loc=5, scale = 2, size = 200), \n",
    "                    'e_r': np.abs(np.random.normal(0, scale = 1, size = 200)), \n",
    "                    'f_i': np.random.normal(loc=5, scale = 2, size = 200), \n",
    "                    'e_i': np.abs(np.random.normal(0, scale = 1, size = 200)), \n",
    "                    'f_z': np.random.normal(loc=5, scale = 2, size = 200), \n",
    "                    'e_z': np.abs(np.random.normal(0, scale = 1, size = 200))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing the first 5 rows and all the columns\n",
    "DF2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat\n",
    "\n",
    "The first function we will cover is the concat short for Concatenate will merge two dataframes vertically and this is super useful when you have two DataFrames with the same columns and is a super usefule way of adding more data to a main DataFrame. Below we show an example of what happens to the DataFrame with the same and different columns names when we use concat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Concat on DF1 and DF2\n",
    "concat_df = pd.concat([DF1, DF2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input 145 and see what we get\n",
    "concat_df.loc[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a DF with similar column names to DF1\n",
    "np.random.seed(43)\n",
    "DF3 = pd.DataFrame({'Field': np.random.choice(DF1.Field.values, size = 100), \n",
    "                    'RA': np.random.uniform(low = 0, high = 360, size = 100), \n",
    "                    'DEC': np.random.uniform(low = -90, high = 90, size = 100)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_concat_df = pd.concat([DF1, DF3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_concat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join\n",
    "The second function we will cover is the *join* function and this is a super useful function when you are taking one dataframe and merging it to another with similar indexes or similar keys. This is great when you have two DataFrames having the same indices/keys but have different column names as this is a way of merging two in a larger DataFrame or down selecting from a larger dataframe only the data that you are after. The way you perform a *join* in the python is using the DataFrames themselves. So if you have a dataframe DF1 and you want to join DF2 you do DF1.join(DF2) where it will match DF2 to DF1.\n",
    "\n",
    "If you want the reverse you would need to reverse the input, ie DF2.join(DF1). Let's see this in action below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying join matching DF2 to DF1\n",
    "joined_DF2_to_DF1 = DF1.join(DF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_DF2_to_DF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying join to DF1 starting from DF2\n",
    "joined_DF1_to_DF2 = DF2.join(DF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_DF1_to_DF2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_DF1_to_DF2.loc[[31, 43, 76, 112, 145, 198, 10, 11, 12, 13]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File IO with Pandas\n",
    "\n",
    "Pandas comes with loads of useful reading functions for all sorts of data types. You can tweak up certain ones to make it read other formats not specified by the function. In the following section we will go over the read_csv function, how to use it to read in both csv files and text files as well as showing you how to save a file using pandas to_csv function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 1\n",
    "DF_FIELD_Coords_ex1 = pd.read_csv('Field_Coordinates.txt', sep = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_FIELD_Coords_ex1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 2\n",
    "DF_FIELD_Coords_ex2 = pd.read_csv('Field_Coordinates.txt', delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_FIELD_Coords_ex2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 3\n",
    "DF_Gal_Coords_ex3 = pd.read_csv('Galaxy_Coordinates.txt', sep = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_Gal_Coords_ex3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 4\n",
    "DF_Gal_Coords_ex4 = pd.read_csv('Galaxy_Coordinates.txt',delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_Gal_Coords_ex4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the data as a text file\n",
    "joined_DF2_to_DF1.to_csv('Final_Sample.txt', sep = ' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
